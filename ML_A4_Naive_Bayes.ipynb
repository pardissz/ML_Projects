{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b07eb33c"
      },
      "source": [
        "<div>\n",
        "<img src='http://www-scf.usc.edu/~ghasemig/images/sharif.png' alt=\"SUT logo\" width=220 height=220 align=left class=\"saturate\">\n",
        "\n",
        "<br>\n",
        "<font face=\"Times New Roman\">\n",
        "<div dir=ltr align=center> \n",
        "<!-- <font color=0F5298 size=7> -->\n",
        "<font color=0F5298 size=6>\n",
        "    Introduction to Machine Learning <br> <br>\n",
        "<!-- <font color=2565AE size=5> -->\n",
        "<font size=5>\n",
        "    Computer Engineering Department <br>\n",
        "    Spring 2023 <br> <br>\n",
        "<font color=606060 size=5>\n",
        "    Homework 4: Practical - Naive Bayes <br> <br>\n",
        "<font color=686880 size=4>\n",
        "    TAs: Alireza Farashah - Arman Malekzadeh - Ali Salesi\n",
        "    \n",
        "____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5a79508"
      },
      "source": [
        "### Full Name : Pardis Zahraei\n",
        "### Student Number : 99109777\n",
        "### Colab Link: https://colab.research.google.com/drive/1TpquJADYHp06dyrwvw4vILUD37wG_y-3?usp=sharing\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note 1: In this assignment, we are trying to simulate the functionality of something called \"CountVectorizer\" which is used in natural language processing. You are advised to take a look at this link before beginning to answer the questions:\n",
        "\n",
        "https://www.geeksforgeeks.org/using-countvectorizer-to-extracting-features-from-text/\n",
        "\n",
        "Note 2: One or two TA sessions will be held related to this assignment to make sure you get familiarized with this topic. Therefore, keep calm and write code!"
      ],
      "metadata": {
        "id": "QTZBzFXNv8pP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the Dataset"
      ],
      "metadata": {
        "id": "Dsp-qF9vLbUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O /content/dataset.zip \"https://www.dropbox.com/s/yf195wl0sp3term/dataset-train.zip?dl=1\""
      ],
      "metadata": {
        "id": "YQG4cjKvK4zZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d89165c4-e4d8-401e-c181-2c4fb83fb30c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-05 19:31:28--  https://www.dropbox.com/s/yf195wl0sp3term/dataset-train.zip?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/dl/yf195wl0sp3term/dataset-train.zip [following]\n",
            "--2023-05-05 19:31:29--  https://www.dropbox.com/s/dl/yf195wl0sp3term/dataset-train.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc317545976517b96a1e5a205b6a.dl.dropboxusercontent.com/cd/0/get/B7fEVzBDlvY09ozCj5GCjtt9pIfg55qO3aPJP_04vJq-s5I3gCLKBhD-QGCu-oR0nig5PabS89tsp1jQ2ru5BbonZYWlGsnblOSQg_hxjAokYyvGwtAAELQqGlfT5KEofTsLpNt9kNlyfI0EoixGxhBj9U1rTc6U7NXB1z45kw7m4xElT0aCUeTdz_isaYoKWvc/file?dl=1# [following]\n",
            "--2023-05-05 19:31:29--  https://uc317545976517b96a1e5a205b6a.dl.dropboxusercontent.com/cd/0/get/B7fEVzBDlvY09ozCj5GCjtt9pIfg55qO3aPJP_04vJq-s5I3gCLKBhD-QGCu-oR0nig5PabS89tsp1jQ2ru5BbonZYWlGsnblOSQg_hxjAokYyvGwtAAELQqGlfT5KEofTsLpNt9kNlyfI0EoixGxhBj9U1rTc6U7NXB1z45kw7m4xElT0aCUeTdz_isaYoKWvc/file?dl=1\n",
            "Resolving uc317545976517b96a1e5a205b6a.dl.dropboxusercontent.com (uc317545976517b96a1e5a205b6a.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:601b:15::a27d:80f\n",
            "Connecting to uc317545976517b96a1e5a205b6a.dl.dropboxusercontent.com (uc317545976517b96a1e5a205b6a.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 170224802 (162M) [application/binary]\n",
            "Saving to: ‘/content/dataset.zip’\n",
            "\n",
            "/content/dataset.zi 100%[===================>] 162.34M   115MB/s    in 1.4s    \n",
            "\n",
            "2023-05-05 19:31:31 (115 MB/s) - ‘/content/dataset.zip’ saved [170224802/170224802]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/dataset/\n",
        "!unzip '/content/dataset.zip' -d /content/dataset/"
      ],
      "metadata": {
        "id": "GkjLDi1xBR-r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfb2a9ea-0056-415e-e7d8-9687322e69f1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/dataset/’: File exists\n",
            "Archive:  /content/dataset.zip\n",
            "replace /content/dataset/content/news-train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "AI_tN2UDLgT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "xzZeY9pcBo-7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the csv file and load it as a dataframe."
      ],
      "metadata": {
        "id": "bovAIGr4LkcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "csv_path = '/content/dataset/content/news-train.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "5XkCekbTQzaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1fef2f1-4b99-44b4-cbfb-afd06168f5f2"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       topic                                               text\n",
            "0     sports  به گزارش ورزش سه تیم فوتبال پدیده در روزی که ا...\n",
            "1   politics  پاسخ مثبت به انتقادات شورای نگهبان البته # قبل...\n",
            "2   politics  # دهم در پاسخ به سوالی درباره اینکه آیا سفر وی...\n",
            "3  economics  کنفرانس # بودن بازار خودرو و تاثیر آن بر اقتصا...\n",
            "4  economics  به گزارش ایسنا صحبت با برخی # # آن است که امرو...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dealing with Class Imbalance"
      ],
      "metadata": {
        "id": "b2eq6fRZZroS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each \"topic\", count and display the number of news belonging to it. (5 points)"
      ],
      "metadata": {
        "id": "__mdmjwgNSRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_counts = df['topic'].value_counts()\n",
        "print(topic_counts)"
      ],
      "metadata": {
        "id": "1f0oIh-PNYLB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e46b45f-0c97-4ee8-c577-a9c735392dad"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sports       123105\n",
            "economics     49600\n",
            "cultural      13359\n",
            "politics      11297\n",
            "Name: topic, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Balance the dataset in a way that all of the topics get associated with the same number of news. For this purpose, find the topic for which the minimum number of news exists. Then, use downsampling to lower the number of news associated with the other topics.<br>\n",
        "Finally, save the result as a new dataframe. (15 points)"
      ],
      "metadata": {
        "id": "8ETq4g97Oj0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_count = df['topic'].value_counts().min()\n",
        "dfs = []\n",
        "for topic in df['topic'].unique():\n",
        "    temp = df[df['topic'] == topic].sample(n=min_count, random_state=42)\n",
        "    dfs.append(temp)\n",
        "balanced_df = pd.concat(dfs)"
      ],
      "metadata": {
        "id": "XJawN5XRDFYf"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation and Feature Extraction"
      ],
      "metadata": {
        "id": "9DMwlMiwZ4Ql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the dataframe to two parts for training (80%) and testing (20%). (5 points)"
      ],
      "metadata": {
        "id": "FyrxWwjXQcfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = train_test_split(balanced_df, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "BXM2X_tJZIJ2"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part, we will extract features from the textual data using a very basic method."
      ],
      "metadata": {
        "id": "tSzLXYMhaL3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the following sentence:\n",
        "```\n",
        "Sometimes a dog can run faster than a cat.\n",
        "```\n",
        "In the above sentence, the \"tokens\" are:\n",
        "\n",
        "```\n",
        "\"sometimes\", \"a\", \"dog\", \"can\", \"run\", \"faster\", \"than\", \"a\", \"cat\"\n",
        "```"
      ],
      "metadata": {
        "id": "BCejmR4nhABv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the tokens of each news article in the \"training dataframe\" by splitting it based on the occurence of the space character (5 points). For instance, the following sentence:\n",
        "```\n",
        "او به میدان رفت\n",
        "```\n",
        "gets converted to these tokens:\n",
        "```\n",
        "[\"او\",\"به\",\"میدان\",\"رفت\"]\n",
        "```"
      ],
      "metadata": {
        "id": "fTnb43swgo8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['tokens'] = train_df['text'].str.split()\n",
        "print(train_df['tokens'][0])"
      ],
      "metadata": {
        "id": "_dlCAqisb6cT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c52374a2-b64e-4c05-bea6-6ad64651ae25"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['به', 'گزارش', 'ورزش', 'سه', 'تیم', 'فوتبال', 'پدیده', 'در', 'روزی', 'که', 'انتظار', 'می\\u200cرفت', 'با', 'توجه', 'به', 'مشکلات', 'و', 'حواشی', 'که', 'در', 'طی', 'هفته', 'گذشته', 'برای', 'نفت', 'مسجد', 'به', 'وجود', 'آمده', 'است', 'به\\u200cراحتی', 'به', 'جمع', 'هشت', 'تیم', 'نهایی', 'صعود', 'کند', '#', 'یک', '#', '#', 'باز', 'شد', 'و', 'در', 'ادامه', 'نتوانستند', '#', 'خورده', 'را', 'جبران', 'کنند', 'تا', 'در', 'شهر', 'مشهد', 'شکست', '#', 'را', 'متحمل', 'شوند', 'یحیی', 'گل\\u200cمحمدی', 'سرمربی', 'تیم', 'فوتبال', 'شهر', 'خودرو', 'پس', 'از', 'پایان', 'بازی', 'در', 'نشست', 'خبری', 'حضور', 'یافت', 'و', 'در', 'ابتدا', 'بیان', 'کرد', 'نتیجه', 'آن', 'چیزی', 'نبود', 'که', '#', 'را', 'داشتیم', '#', 'جام', 'حذفی', 'جام', 'کم', 'اشتباه', 'بودن', 'است', 'یک', 'اشتباه', '#', 'داشتیم', 'و', 'با', 'گلی', 'که', '#', 'جریمه', 'شدیم', 'حریف', 'با', '#', 'دفاع', 'منظم', 'و', 'فشرده', 'کار', 'را', 'سخت', 'کرد', 'در', 'این', 'مسابقات', 'باید', 'از', 'تک', '#', 'استفاده', 'کنید', 'در', 'ادامه', 'هم', 'فشار', 'زیادی', 'آوردیم', 'اما', 'نشد', 'که', 'به', 'گل', 'برسیم', 'فکر', 'می\\u200cکنم', 'بازیکنان', 'تلاش', 'فراوانی', 'کردند', 'اما', 'نشد', 'برای', 'نفت', 'مسجدسلیمان', 'آرزوی', 'موفقیت', 'در', 'ادامه', 'راه', 'می\\u200cکنیم', 'گل\\u200cمحمدی', 'در', 'خصوص', 'شرایط', 'تیمش', 'گفت', 'در', 'لیگ', 'ایران', 'همیشه', 'وقفه', 'بوده', 'هست', 'و', 'خواهد', 'بود', 'این', 'شرایط', 'را', 'همیشه', 'فوتبال', 'ما', 'داشته', 'به', 'هر', 'صورت', 'روزهای', '#', 'است', 'و', 'کاری', 'نمی\\u200cشد', 'کرد', 'با', 'تیم', 'ملی', 'امید', 'بازی', 'کردیم', 'تا', '#', '#', 'را', 'حفظ', 'کنیم', 'بعد', 'از', 'چند', 'برد', 'وقتی', 'وقفه', 'به', 'وجود', 'می\\u200cآید', 'کار', 'سخت', 'می\\u200cشود', 'باید', 'از', 'این', 'بازی\\u200cها', 'برای', 'لیگ', 'استفاده', 'کنیم', 'تا', 'نقاط', '#', 'را', 'شناسایی', 'کنیم', 'سرمربی', 'شهرخودرو', 'خراسان', 'در', 'خصوص', 'داوری', 'مسابقه', 'اظهار', 'کرد', '#', 'به', 'زود', 'پرچم', 'زدن', 'کمک', 'داور', 'بود', 'آنچه', 'ما', 'می\\u200cدانیم', 'این', 'بود', 'که', 'باید', 'با', 'تامل', 'بیشتری', 'پرچم', '#', 'شود', 'تیم', 'داوری', 'در', 'مجموع', 'قضاوت', 'خوبی', 'داشت', 'و', 'به', 'مهدوی', '#', 'می\\u200cگویم', 'هنوز', 'برای', 'صحبت', 'درباره', 'آسیا', 'زود', 'است', '#', 'که', 'تا', 'آن', 'موقع', 'در', 'لیگ', 'داریم', 'به', 'ما', 'کمک', 'می\\u200cکند', 'که', 'نقاط', 'ضعف', 'و', 'قوت', 'خود', 'را', '#', 'تمام', 'تیم\\u200cهایی', 'که', 'اینجا', 'مقابل', 'ما', 'بازی', 'می\\u200cکنند', '#', 'نفره', 'دفاع', 'می\\u200cکنند', 'خیلی', 'از', 'تیم\\u200cها', 'هم', 'با', 'وجود', 'دفاع', '#', 'نفره', 'مقابل', 'ما', 'شکست', '#', 'در', 'یک', 'بازی', 'می\\u200cتوانیم', 'با', 'تصمیمات', 'خوب', 'در', 'حمله', 'پیروز', 'شویم', 'اما', 'یک', 'بازی', 'هم', 'حریف', 'بهتر', 'دفاع', 'می\\u200cکند', 'انتظار', 'برد', '#', 'در', 'همه', '#', 'که', 'در', 'ورزشگاه', 'امام', 'رضا', 'ع', 'برگزار', 'می\\u200cشود', 'اشتباه', 'است', 'گل\\u200cمحمدی', 'در', 'خصوص', 'تاخیر', '#', 'در', 'بازی', 'گفت', 'این', 'اتفاقات', 'عجیب', 'در', 'فوتبال', 'ایران', 'رخ', 'می\\u200cدهد', 'نمی\\u200cدانم', 'چه', 'باید', 'بگویم', 'گفتند', 'گویا', 'به', 'خاطر', '#', 'این', 'بازی', 'با', 'تاخیر', 'آغاز', 'شد', 'که', 'خب', 'طبیعتا', 'در', 'کار', 'ما', 'هم', 'اختلال', 'ایجاد', 'کرد']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count the tokens and list the top 5 frequent ones (5 points)"
      ],
      "metadata": {
        "id": "eD-UebxhiUyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "tokens_list = [token for tokens in train_df['tokens'] for token in tokens]\n",
        "token_c = Counter(tokens_list)\n",
        "top_tokens = token_c.most_common(5)\n",
        "for token, count in top_tokens:\n",
        "    print(f\"{token}: {count}\")"
      ],
      "metadata": {
        "id": "rg-aJqkSiEJb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64c4f7e4-5bac-46cf-fb98-fdbb7d4d0bf8"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#: 1621876\n",
            "و: 878306\n",
            "در: 743093\n",
            "به: 580896\n",
            "از: 435477\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the following sentence:\n",
        "```\n",
        "تیم فوتبال بارسلونا امشب با بایرن مونیخ دیدار می‌کند.\n",
        "```\n",
        "In the above sentence, we do not need the word `با` to determine that the topic is \"sports\". This word doesn't have any impact on the category this sentence belongs to. \n",
        "This word and similar ones are called \"stopwords\". A list of stopwords for the Persian language can be found in this text file:\n",
        "https://github.com/ziaa/Persian-stopwords-collection/raw/master/Stopwords/Savoy/persianST.txt\n",
        "\n",
        "Use it to remove all tokens that are actually stopwords.\n",
        "Also, remove the `#` token. (10 points)"
      ],
      "metadata": {
        "id": "dJyE1s6Wi0ZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O /content/stopwords.txt https://github.com/ziaa/Persian-stopwords-collection/raw/master/Stopwords/Savoy/persianST.txt"
      ],
      "metadata": {
        "id": "6xV_zvIvjtH1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7b89d8a-139e-404d-dd99-745a99bc9f94"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-05 19:32:18--  https://github.com/ziaa/Persian-stopwords-collection/raw/master/Stopwords/Savoy/persianST.txt\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ziaa/Persian-stopwords-collection/master/Stopwords/Savoy/persianST.txt [following]\n",
            "--2023-05-05 19:32:19--  https://raw.githubusercontent.com/ziaa/Persian-stopwords-collection/master/Stopwords/Savoy/persianST.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2942 (2.9K) [text/plain]\n",
            "Saving to: ‘/content/stopwords.txt’\n",
            "\n",
            "/content/stopwords. 100%[===================>]   2.87K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-05-05 19:32:19 (8.12 MB/s) - ‘/content/stopwords.txt’ saved [2942/2942]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/stopwords.txt', 'r') as f:\n",
        "    stopwords = set(f.read().splitlines())\n",
        "def remove_stopwords(tokens):\n",
        "    return [token for token in tokens if token not in stopwords and token != '#']\n",
        "train_df['real_tokens'] = train_df['tokens'].apply(remove_stopwords)\n",
        "print(train_df['real_tokens'][0])"
      ],
      "metadata": {
        "id": "w5TLeugQjxIt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4c02a7b-3fcb-4639-cea0-d30f829acfe3"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['گزارش', 'ورزش', 'سه', 'تیم', 'فوتبال', 'پدیده', 'روزی', 'انتظار', 'می\\u200cرفت', 'توجه', 'مشکلات', 'حواشی', 'طی', 'هفته', 'گذشته', 'برای', 'نفت', 'مسجد', 'وجود', 'به\\u200cراحتی', 'جمع', 'هشت', 'تیم', 'نهایی', 'صعود', 'کند', 'باز', 'ادامه', 'نتوانستند', 'خورده', 'جبران', 'کنند', 'شهر', 'مشهد', 'شکست', 'متحمل', 'یحیی', 'گل\\u200cمحمدی', 'سرمربی', 'تیم', 'فوتبال', 'شهر', 'خودرو', 'پایان', 'بازی', 'نشست', 'خبری', 'حضور', 'یافت', 'ابتدا', 'بیان', 'کرد', 'نتیجه', 'داشتیم', 'جام', 'حذفی', 'جام', 'کم', 'اشتباه', 'اشتباه', 'داشتیم', 'گلی', 'جریمه', 'شدیم', 'حریف', 'دفاع', 'منظم', 'فشرده', 'کار', 'سخت', 'کرد', 'این', 'مسابقات', 'باید', 'تک', 'کنید', 'ادامه', 'فشار', 'زیادی', 'آوردیم', 'نشد', 'گل', 'برسیم', 'فکر', 'می\\u200cکنم', 'بازیکنان', 'تلاش', 'فراوانی', 'کردند', 'نشد', 'برای', 'نفت', 'مسجدسلیمان', 'آرزوی', 'موفقیت', 'ادامه', 'می\\u200cکنیم', 'گل\\u200cمحمدی', 'خصوص', 'شرایط', 'تیمش', 'لیگ', 'ایران', 'همیشه', 'وقفه', 'این', 'شرایط', 'همیشه', 'فوتبال', 'روزهای', 'کاری', 'نمی\\u200cشد', 'کرد', 'تیم', 'ملی', 'امید', 'بازی', 'کردیم', 'حفظ', 'کنیم', 'برد', 'وقتی', 'وقفه', 'وجود', 'می\\u200cآید', 'کار', 'سخت', 'می\\u200cشود', 'باید', 'این', 'بازی\\u200cها', 'برای', 'لیگ', 'کنیم', 'نقاط', 'شناسایی', 'کنیم', 'سرمربی', 'شهرخودرو', 'خراسان', 'خصوص', 'داوری', 'مسابقه', 'اظهار', 'کرد', 'زود', 'پرچم', 'زدن', 'کمک', 'داور', 'می\\u200cدانیم', 'این', 'باید', 'تامل', 'بیشتری', 'پرچم', 'تیم', 'داوری', 'مجموع', 'قضاوت', 'خوبی', 'مهدوی', 'می\\u200cگویم', 'برای', 'صحبت', 'آسیا', 'زود', 'موقع', 'لیگ', 'داریم', 'کمک', 'می\\u200cکند', 'نقاط', 'ضعف', 'قوت', 'تیم\\u200cهایی', 'اینجا', 'بازی', 'می\\u200cکنند', 'نفره', 'دفاع', 'می\\u200cکنند', 'خیلی', 'تیم\\u200cها', 'وجود', 'دفاع', 'نفره', 'شکست', 'بازی', 'می\\u200cتوانیم', 'تصمیمات', 'خوب', 'حمله', 'پیروز', 'شویم', 'بازی', 'حریف', 'بهتر', 'دفاع', 'می\\u200cکند', 'انتظار', 'برد', 'ورزشگاه', 'امام', 'رضا', 'ع', 'برگزار', 'می\\u200cشود', 'اشتباه', 'گل\\u200cمحمدی', 'خصوص', 'تاخیر', 'بازی', 'این', 'اتفاقات', 'عجیب', 'فوتبال', 'ایران', 'رخ', 'می\\u200cدهد', 'نمی\\u200cدانم', 'باید', 'بگویم', 'گفتند', 'گویا', 'خاطر', 'این', 'بازی', 'تاخیر', 'آغاز', 'خب', 'طبیعتا', 'کار', 'اختلال', 'ایجاد', 'کرد']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make a list of the remaining unique tokens (5 points)"
      ],
      "metadata": {
        "id": "WH6Lw66Xk-Vb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "real_tokens_list = [token for tokens in train_df['real_tokens'] for token in tokens]\n",
        "unique_tokens = set(real_tokens_list)\n",
        "print(\"First 10 unique tokens:\")\n",
        "for token in list(unique_tokens)[:10]:\n",
        "    print(token)"
      ],
      "metadata": {
        "id": "ASLpUFZgkROF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55ef75ee-6e2c-4afd-b91b-806fe37208bd"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 unique tokens:\n",
            "خورشید\n",
            "وقت\n",
            "اطلاع‌رسانی\n",
            "مطالبات\n",
            "لیورپول\n",
            "عامل\n",
            "ریسک\n",
            "نماد\n",
            "مولد\n",
            "انتقادی\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count the number of the occurences of each token in each document. This way, you can make a numpy array to represent each document. Call this new array `x_train`. (10 points)\n",
        "\n",
        "Example:\n",
        "\n",
        "Unique tokens in all documents: \n",
        "```\n",
        "0: hen\n",
        "1: sometimes\n",
        "2: a\n",
        "3: the\n",
        "4: dog\n",
        "5: runs\n",
        "6: faster\n",
        "7: than\n",
        "8: cat\n",
        "9: bird\n",
        "```\n",
        "\n",
        "Current Document: `sometimes a dog runs faster than a cat`\n",
        "\n",
        "The representation of the document:\n",
        "\n",
        "```\n",
        "[0, 1, 2, 0, 1, 1, 1, 1, 1, 0]\n",
        "```\n",
        "\n",
        "The meaning of this representation:\n",
        "\n",
        "```\n",
        "0: the document doesn't contain \"hen\"\n",
        "1: the document contains 1 \"sometimes\"\n",
        "2: the document contains 2 \"a\"\n",
        "0: the document doesn't contain \"the\"\n",
        "1: the document contains 1 \"dog\"\n",
        "1: the document contains 1 \"runs\"\n",
        "1: the document contains 1 \"faster\"\n",
        "1: the document contains 1 \"than\"\n",
        "1: the document contains 1 \"cat\"\n",
        "0: the document doesn't contain \"bird\"\n",
        "``` "
      ],
      "metadata": {
        "id": "xI9kbGYql-WU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "train_df['real_text'] = train_df['real_tokens'].apply(' '.join)\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(train_df['real_text'])\n",
        "s_xtrain = vectorizer.transform(train_df['real_text'])\n",
        "x_train = s_xtrain.toarray()"
      ],
      "metadata": {
        "id": "Z17IkdW-lc2p"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make another numpy array by converting the topics associated with the training news to numbers. For instance, if the topics are: `sports`, `economics`, `politics`, and `cultural`, convert them to `0` to `3`. Call this new array `y_train`. (5 points)"
      ],
      "metadata": {
        "id": "NXeW76p2pjnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(train_df['topic'])\n",
        "y_train = label_encoder.transform(train_df['topic'])"
      ],
      "metadata": {
        "id": "wV5zeofEF3Nn"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the unique tokens you found in the training news, represent the testing news as another numpy array called `x_test`. (5 points)\n",
        "\n",
        "Note: Do not forget to remove stopwords and the `#` token."
      ],
      "metadata": {
        "id": "lew_jlNPqhPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df['tokens'] = test_df['text'].str.split()\n",
        "test_df['real_tokens'] = test_df['tokens'].apply(remove_stopwords)\n",
        "test_df['real_text'] = test_df['real_tokens'].apply(' '.join)\n",
        "s_xtest = vectorizer.transform(test_df['real_text'])\n",
        "x_test = s_xtest.toarray()"
      ],
      "metadata": {
        "id": "RIy2qMABrQcy"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the topics associated with the testing news to their equivalent numbers (as before), and save the result as a numpy array called `y_test`. (5 points)"
      ],
      "metadata": {
        "id": "QWIRhxS_rX7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = label_encoder.transform(test_df['topic'])"
      ],
      "metadata": {
        "id": "T0tXlWRHrWUI"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training and Evaluation"
      ],
      "metadata": {
        "id": "LQdf4UuQrvme"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using sklearn (10 points)"
      ],
      "metadata": {
        "id": "MwkWJbTEsoWy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a Multinomial Naive Bayes Model on the training news (`x_train` and `y_train`) using `sklearn`."
      ],
      "metadata": {
        "id": "sGcrjKBgr1uL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "yJTD0NB7rW2P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "b8353ca6-4089-4fd1-8d21-565484a5009f"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the predictions of the model for the testing news (`x_test`)."
      ],
      "metadata": {
        "id": "OeFDzw61sCna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = nb_model.predict(x_test)"
      ],
      "metadata": {
        "id": "f0KP-84CsCJX"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the classification report containing \"precision\", \"recall\", and \"f1-score\" for each class, and their averages. (you can use `sklearn` for this part)"
      ],
      "metadata": {
        "id": "G1KAXbhJsb5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "1fLYOmgMsWns",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2b1bc61-b7b7-4333-8f27-3ca21a47b9a1"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.95      0.94      2255\n",
            "           1       0.91      0.94      0.93      2268\n",
            "           2       0.94      0.90      0.92      2262\n",
            "           3       0.99      0.98      0.99      2253\n",
            "\n",
            "    accuracy                           0.95      9038\n",
            "   macro avg       0.95      0.95      0.95      9038\n",
            "weighted avg       0.95      0.95      0.95      9038\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using your own code (15 points)"
      ],
      "metadata": {
        "id": "dM7sjbu6stRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NaiveBayes:\n",
        "    def __init__(self):\n",
        "        self.classes = None\n",
        "        self.priors = None\n",
        "        self.c_probs = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes = np.unique(y)\n",
        "        self.priors = np.zeros(len(self.classes))\n",
        "        for i, c in enumerate(self.classes):\n",
        "            self.priors[i] = np.mean(y == c)\n",
        "        self.c_probs = []\n",
        "        for i, c in enumerate(self.classes):\n",
        "            X_c = X[y == c]\n",
        "            word_c = np.sum(X_c, axis=0)\n",
        "            cond_probs = (word_c + 1) / (np.sum(word_c) + len(word_c))\n",
        "            self.c_probs.append(cond_probs)\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = []\n",
        "        for x in X:\n",
        "            log_probs = []\n",
        "            for i, c in enumerate(self.classes):\n",
        "                log_prob = np.log(self.priors[i]) + np.sum(np.log(self.c_probs[i]) * x)\n",
        "                log_probs.append(log_prob)\n",
        "            y_pred.append(self.classes[np.argmax(log_probs)])\n",
        "        return np.array(y_pred)"
      ],
      "metadata": {
        "id": "HEphy8nZ_7hQ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train your own Multinomial Naive Bayes Model on the training news (`x_train` and `y_train`)."
      ],
      "metadata": {
        "id": "ZYpwLACxtlFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb = NaiveBayes()\n",
        "nb.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "QBnzQoqyIl-q"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the predictions of your model for the testing news (`x_test`)."
      ],
      "metadata": {
        "id": "pPSZVHBCtndS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = nb.predict(x_test)"
      ],
      "metadata": {
        "id": "T7SMtuR0tskf"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the classification report containing \"precision\", \"recall\", and \"f1-score\" for each class, and their averages. (you can use `sklearn` for this part)"
      ],
      "metadata": {
        "id": "hIlERTP1ttCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "9MfxJtbmtugx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a82e349-efb8-4a88-dfd2-cac09c11e929"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.95      0.94      2255\n",
            "           1       0.91      0.94      0.93      2268\n",
            "           2       0.94      0.90      0.92      2262\n",
            "           3       0.99      0.98      0.99      2253\n",
            "\n",
            "    accuracy                           0.95      9038\n",
            "   macro avg       0.95      0.95      0.95      9038\n",
            "weighted avg       0.95      0.95      0.95      9038\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kaggle"
      ],
      "metadata": {
        "id": "ElMEcOMAeFA1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Competition Link: https://www.kaggle.com/t/88f1b6e251e34575b2e4cb4b91aed0ef"
      ],
      "metadata": {
        "id": "64J1wFXweGsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#my code\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "csv_path = '/content/dataset/content/news-train.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "min_count = df['topic'].value_counts().min()\n",
        "dfs = []\n",
        "for topic in df['topic'].unique():\n",
        "    temp_df = df[df['topic'] == topic].sample(n=min_count, random_state=42)\n",
        "    dfs.append(temp_df)\n",
        "balanced_df = pd.concat(dfs)\n",
        "train_df, test_df = train_test_split(balanced_df, test_size=0.2, random_state=42)\n",
        "train_df['tokens'] = train_df['text'].str.split()\n",
        "from collections import Counter\n",
        "tokens_list = [token for tokens in train_df['tokens'] for token in tokens]\n",
        "with open('/content/stopwords.txt', 'r') as f:\n",
        "    stopwords = set(f.read().splitlines())\n",
        "def remove_stopwords(tokens):\n",
        "    return [token for token in tokens if token not in stopwords and token != '#']\n",
        "train_df['real_tokens'] = train_df['tokens'].apply(remove_stopwords)\n",
        "real_tokens_list = [token for tokens in train_df['real_tokens'] for token in tokens]\n",
        "unique_tokens = set(real_tokens_list)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "train_df['real_text'] = train_df['real_tokens'].apply(' '.join)\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(train_df['real_text'])\n",
        "s_xtrain = vectorizer.transform(train_df['real_text'])\n",
        "x_train = s_xtrain.toarray()\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(train_df['topic'])\n",
        "y_train = label_encoder.transform(train_df['topic'])\n",
        "test_df['tokens'] = test_df['text'].str.split()\n",
        "test_df['real_tokens'] = test_df['tokens'].apply(remove_stopwords)\n",
        "test_df['real_text'] = test_df['real_tokens'].apply(' '.join)\n",
        "s_xtest = vectorizer.transform(test_df['real_text'])\n",
        "x_test = s_xtest.toarray()\n",
        "y_test = label_encoder.transform(test_df['topic'])\n",
        "nb = NaiveBayes()\n",
        "nb.fit(x_train, y_train)\n",
        "y_pred = nb.predict(x_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "id": "fhcGDVJTAw7p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dfc87c5-5c8f-45e2-b15a-afb1a81aecc4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.95      0.94      2255\n",
            "           1       0.91      0.94      0.93      2268\n",
            "           2       0.94      0.90      0.92      2262\n",
            "           3       0.99      0.98      0.99      2253\n",
            "\n",
            "    accuracy                           0.95      9038\n",
            "   macro avg       0.95      0.95      0.95      9038\n",
            "weighted avg       0.95      0.95      0.95      9038\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv('assignment4-test-data.csv')\n",
        "df_test = df_test.drop([\"ID\"], axis=1)\n",
        "df_test['tokens'] = df_test['text'].str.split()\n",
        "df_test['real_tokens'] = df_test['tokens'].apply(remove_stopwords)\n",
        "df_test['real_text'] = df_test['real_tokens'].apply(' '.join)\n",
        "s_xtest = vectorizer.transform(df_test['real_text'])\n",
        "x_test = s_xtest.toarray()\n",
        "y_pred = nb.predict(x_test)\n",
        "predicted_topics = label_encoder.inverse_transform(y_pred)\n",
        "def map_topic(topic):\n",
        "    if topic == 'politics':\n",
        "        return 1\n",
        "    elif topic == 'economics':\n",
        "        return 2\n",
        "    elif topic == 'sports':\n",
        "        return 3\n",
        "    elif topic == 'cultural':\n",
        "        return 4\n",
        "mapped_topics = np.vectorize(map_topic)(predicted_topics)\n",
        "id_arr=np.arange(1,501)\n",
        "print(len(mapped_topics))\n",
        "print(len(id_arr))\n",
        "final_df=pd.DataFrame({'topic':mapped_topics,'ID':id_arr })\n",
        "final_df.to_csv('cc.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DP5Zq9HANoH1",
        "outputId": "86293f4b-9a55-4e2f-92ee-5b121d9af23c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500\n",
            "500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sklearn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "csv_path = '/content/dataset/content/news-train.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "min_count = df['topic'].value_counts().min()\n",
        "dfs = []\n",
        "for topic in df['topic'].unique():\n",
        "    temp_df = df[df['topic'] == topic].sample(n=min_count, random_state=42)\n",
        "    dfs.append(temp_df)\n",
        "balanced_df = pd.concat(dfs)\n",
        "train_df, test_df = train_test_split(balanced_df, test_size=0.2, random_state=42)\n",
        "train_df['tokens'] = train_df['text'].str.split()\n",
        "from collections import Counter\n",
        "tokens_list = [token for tokens in train_df['tokens'] for token in tokens]\n",
        "with open('/content/stopwords.txt', 'r') as f:\n",
        "    stopwords = set(f.read().splitlines())\n",
        "def remove_stopwords(tokens):\n",
        "    return [token for token in tokens if token not in stopwords and token != '#']\n",
        "train_df['real_tokens'] = train_df['tokens'].apply(remove_stopwords)\n",
        "real_tokens_list = [token for tokens in train_df['real_tokens'] for token in tokens]\n",
        "unique_tokens = set(real_tokens_list)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "train_df['real_text'] = train_df['real_tokens'].apply(' '.join)\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(train_df['real_text'])\n",
        "s_xtrain = vectorizer.transform(train_df['real_text'])\n",
        "x_train = s_xtrain.toarray()\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(train_df['topic'])\n",
        "y_train = label_encoder.transform(train_df['topic'])\n",
        "test_df['tokens'] = test_df['text'].str.split()\n",
        "test_df['real_tokens'] = test_df['tokens'].apply(remove_stopwords)\n",
        "test_df['real_text'] = test_df['real_tokens'].apply(' '.join)\n",
        "s_xtest = vectorizer.transform(test_df['real_text'])\n",
        "x_test = s_xtest.toarray()\n",
        "y_test = label_encoder.transform(test_df['topic'])\n",
        "nb_mult = MultinomialNB()\n",
        "nb_mult.fit(x_train, y_train)\n",
        "y_pred = nb_mult.predict(x_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqSQRDduO_Bl",
        "outputId": "3dd1e060-edbc-4dc0-91b8-48ac40cac8e0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.95      0.94      2255\n",
            "           1       0.91      0.94      0.93      2268\n",
            "           2       0.94      0.90      0.92      2262\n",
            "           3       0.99      0.98      0.99      2253\n",
            "\n",
            "    accuracy                           0.95      9038\n",
            "   macro avg       0.95      0.95      0.95      9038\n",
            "weighted avg       0.95      0.95      0.95      9038\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv('assignment4-test-data.csv')\n",
        "df_test = df_test.drop([\"ID\"], axis=1)\n",
        "df_test['tokens'] = df_test['text'].str.split()\n",
        "df_test['real_tokens'] = df_test['tokens'].apply(remove_stopwords)\n",
        "df_test['real_text'] = df_test['real_tokens'].apply(' '.join)\n",
        "s_xtest = vectorizer.transform(df_test['real_text'])\n",
        "x_test = s_xtest.toarray()\n",
        "y_pred = nb.predict(x_test)\n",
        "predicted_topics = label_encoder.inverse_transform(y_pred)\n",
        "def map_topic(topic):\n",
        "    if topic == 'politics':\n",
        "        return 1\n",
        "    elif topic == 'economics':\n",
        "        return 2\n",
        "    elif topic == 'sports':\n",
        "        return 3\n",
        "    elif topic == 'cultural':\n",
        "        return 4\n",
        "mapped_topics = np.vectorize(map_topic)(predicted_topics)\n",
        "id_arr=np.arange(1,501)\n",
        "print(len(mapped_topics))\n",
        "print(len(id_arr))\n",
        "final_df=pd.DataFrame({'topic':mapped_topics,'ID':id_arr })\n",
        "final_df.to_csv('cc_new.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SamYnbKqPM6u",
        "outputId": "55cb1216-923b-4cc0-9706-85cc0cd8c454"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500\n",
            "500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#my code\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from collections import Counter\n",
        "csv_path = '/content/dataset/content/news-train.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "min_count = df['topic'].value_counts().min()\n",
        "dfs = []\n",
        "for topic in df['topic'].unique():\n",
        "    temp_df = df[df['topic'] == topic].sample(n=min_count, random_state=42)\n",
        "    dfs.append(temp_df)\n",
        "balanced_df = pd.concat(dfs)\n",
        "train_df, test_df = train_test_split(balanced_df, test_size=0.2, random_state=42)\n",
        "train_df['tokens'] = train_df['text'].str.split()\n",
        "from collections import Counter\n",
        "tokens_list = [token for tokens in train_df['tokens'] for token in tokens]\n",
        "with open('/content/stopwords.txt', 'r') as f:\n",
        "    stopwords = set(f.read().splitlines())\n",
        "def remove_stopwords(tokens):\n",
        "    return [token for token in tokens if token not in stopwords and token != '#']\n",
        "train_df['real_tokens'] = train_df['tokens'].apply(remove_stopwords)\n",
        "real_tokens_list = [token for tokens in train_df['real_tokens'] for token in tokens]\n",
        "unique_tokens = set(real_tokens_list)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "train_df['real_text'] = train_df['real_tokens'].apply(' '.join)\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(train_df['real_text'])\n",
        "s_xtrain = vectorizer.transform(train_df['real_text'])\n",
        "x_train = s_xtrain.toarray()\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(train_df['topic'])\n",
        "y_train = label_encoder.transform(train_df['topic'])\n",
        "test_df['tokens'] = test_df['text'].str.split()\n",
        "test_df['real_tokens'] = test_df['tokens'].apply(remove_stopwords)\n",
        "test_df['real_text'] = test_df['real_tokens'].apply(' '.join)\n",
        "s_xtest = vectorizer.transform(test_df['real_text'])\n",
        "x_test = s_xtest.toarray()\n",
        "y_test = label_encoder.transform(test_df['topic'])\n",
        "nb = NaiveBayes()\n",
        "nb.fit(x_train, y_train)\n",
        "y_pred = nb.predict(x_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e0GjkvVPIEm",
        "outputId": "70b1695c-5062-481f-f413-e6be62e10e13"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.95      0.95      2255\n",
            "           1       0.91      0.94      0.93      2268\n",
            "           2       0.94      0.90      0.92      2262\n",
            "           3       0.99      0.98      0.99      2253\n",
            "\n",
            "    accuracy                           0.95      9038\n",
            "   macro avg       0.95      0.95      0.95      9038\n",
            "weighted avg       0.95      0.95      0.95      9038\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv('assignment4-test-data.csv')\n",
        "df_test = df_test.drop([\"ID\"], axis=1)\n",
        "df_test['tokens'] = df_test['text'].str.split()\n",
        "df_test['real_tokens'] = df_test['tokens'].apply(remove_stopwords)\n",
        "df_test['real_text'] = df_test['real_tokens'].apply(' '.join)\n",
        "s_xtest = vectorizer.transform(df_test['real_text'])\n",
        "x_test = s_xtest.toarray()\n",
        "y_pred = nb.predict(x_test)\n",
        "predicted_topics = label_encoder.inverse_transform(y_pred)\n",
        "def map_topic(topic):\n",
        "    if topic == 'politics':\n",
        "        return 1\n",
        "    elif topic == 'economics':\n",
        "        return 2\n",
        "    elif topic == 'sports':\n",
        "        return 3\n",
        "    elif topic == 'cultural':\n",
        "        return 4\n",
        "mapped_topics = np.vectorize(map_topic)(predicted_topics)\n",
        "id_arr=np.arange(1,501)\n",
        "print(len(mapped_topics))\n",
        "print(len(id_arr))\n",
        "final_df=pd.DataFrame({'topic':mapped_topics,'ID':id_arr })\n",
        "final_df.to_csv('ff.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2eTy-i9PItG",
        "outputId": "121b630c-8931-4c9a-dc59-6214c32c5ec0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500\n",
            "500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "csv_path = '/content/dataset/content/news-train.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "min_count = df['topic'].value_counts().min()\n",
        "dfs = []\n",
        "for topic in df['topic'].unique():\n",
        "    temp_df = df[df['topic'] == topic].sample(n=min_count, random_state=42)\n",
        "    dfs.append(temp_df)\n",
        "balanced_df = pd.concat(dfs)\n",
        "train_df, test_df = train_test_split(balanced_df, test_size=0.2, random_state=42)\n",
        "train_df['tokens'] = train_df['text'].str.split()\n",
        "from collections import Counter\n",
        "tokens_list = [token for tokens in train_df['tokens'] for token in tokens]\n",
        "with open('/content/stopwords.txt', 'r') as f:\n",
        "    stopwords = set(f.read().splitlines())\n",
        "def remove_stopwords(tokens):\n",
        "    return [token for token in tokens if token not in stopwords and token != '#']\n",
        "train_df['real_tokens'] = train_df['tokens'].apply(remove_stopwords)\n",
        "real_tokens_list = [token for tokens in train_df['real_tokens'] for token in tokens]\n",
        "unique_tokens = set(real_tokens_list)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "train_df['real_text'] = train_df['real_tokens'].apply(' '.join)\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(train_df['real_text'])\n",
        "s_xtrain = vectorizer.transform(train_df['real_text'])\n",
        "x_train = s_xtrain.toarray()\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(train_df['topic'])\n",
        "y_train = label_encoder.transform(train_df['topic'])\n",
        "test_df['tokens'] = test_df['text'].str.split()\n",
        "test_df['real_tokens'] = test_df['tokens'].apply(remove_stopwords)\n",
        "test_df['real_text'] = test_df['real_tokens'].apply(' '.join)\n",
        "s_xtest = vectorizer.transform(test_df['real_text'])\n",
        "x_test = s_xtest.toarray()\n",
        "y_test = label_encoder.transform(test_df['topic'])\n",
        "param_grid = {'alpha': np.arange(0.1, 1.1, 0.1)}\n",
        "nb = MultinomialNB()\n",
        "grid_search = GridSearchCV(estimator=nb, param_grid=param_grid, cv=5)\n",
        "grid_search.fit(x_train, y_train)\n",
        "best_alpha = grid_search.best_params_['alpha']\n",
        "nb = MultinomialNB(alpha=best_alpha)\n",
        "nb.fit(x_train, y_train)\n",
        "y_pred = nb.predict(x_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "df_test = pd.read_csv('assignment4-test-data.csv')\n",
        "df_test = df_test.drop([\"ID\"], axis=1)\n",
        "df_test['tokens'] = df_test['text'].str.split()\n",
        "df_test['real_tokens'] = df_test['tokens'].apply(remove_stopwords)\n",
        "df_test['real_text'] = df_test['real_tokens'].apply(' '.join)\n",
        "s_xtest = vectorizer.transform(df_test['real_text'])\n",
        "x_test = s_xtest.toarray()\n",
        "y_pred = nb.predict(x_test)\n",
        "predicted_topics = label_encoder.inverse_transform(y_pred)\n",
        "def map_topic(topic):\n",
        "    if topic == 'politics':\n",
        "        return 1\n",
        "    elif topic == 'economics':\n",
        "        return 2\n",
        "    elif topic == 'sports':\n",
        "        return 3\n",
        "    elif topic == 'cultural':\n",
        "        return 4\n",
        "mapped_topics = np.vectorize(map_topic)(predicted_topics)\n",
        "id_arr=np.arange(1,501)\n",
        "print(len(mapped_topics))\n",
        "print(len(id_arr))\n",
        "final_df=pd.DataFrame({'topic':mapped_topics,'ID':id_arr })\n",
        "final_df.to_csv('eee.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Te5Is6uURiIm",
        "outputId": "a43e1af1-1113-413e-dd83-2b99680a654b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.95      0.95      2255\n",
            "           1       0.91      0.94      0.93      2268\n",
            "           2       0.94      0.90      0.92      2262\n",
            "           3       0.99      0.98      0.99      2253\n",
            "\n",
            "    accuracy                           0.95      9038\n",
            "   macro avg       0.95      0.95      0.95      9038\n",
            "weighted avg       0.95      0.95      0.95      9038\n",
            "\n",
            "500\n",
            "500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(best_alpha)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6B7Pdm5WHMl",
        "outputId": "7e3d3fdf-3fe2-4223-f1a3-3b6885e82500"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1\n"
          ]
        }
      ]
    }
  ]
}